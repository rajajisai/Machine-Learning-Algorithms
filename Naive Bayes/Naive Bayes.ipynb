{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31eb6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R SAI RAJAJI-2017B4A70550H\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc44eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1 accuracy 80.28169014084507\n",
      "FOLD 2 accuracy 76.76056338028168\n",
      "FOLD 3 accuracy 80.28169014084507\n",
      "FOLD 4 accuracy 79.5774647887324\n",
      "FOLD 5 accuracy 79.5774647887324\n",
      "FOLD 6 accuracy 79.5774647887324\n",
      "FOLD 7 accuracy 83.09859154929578\n",
      "The average accuracy is  79.87927565392354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "stopwords = frozenset([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "    \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\",\n",
    "    \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"doesn,t\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\n",
    "    \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\",\n",
    "    \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n",
    "    \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\",\n",
    "    \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\",\n",
    "    \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\",\n",
    "    \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\",\n",
    "    \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\",\n",
    "    \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\",\n",
    "    \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\",\n",
    "    \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\",\n",
    "    \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"\"])\n",
    "\n",
    "bad_chars = [';', ':', '!', \"*\", \"(\", \")\", \"\\\"\", \".\", \",\", \"  \", \"-\", \"?\", \"$\", \"[\", \"]\", \"/\", \"\\\\\", \"'\", \"1\", \"2\", \"3\",\n",
    "             \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "#Reading file\n",
    "f = open(\"dataset_NB.txt\", \"r\")\n",
    "t = f.read().splitlines()\n",
    "f.close()\n",
    "#Shuffling\n",
    "a = random.sample(t, len(t))\n",
    "x = []\n",
    "X = []\n",
    "Y = []\n",
    "for i in a:\n",
    "    x.append(i[:-1])\n",
    "    Y.append(i[-1])\n",
    "\n",
    "x = [i.lower() for i in x]\n",
    "\n",
    "for i in x:\n",
    "    for j in bad_chars:\n",
    "        i = i.replace(j, ' ')\n",
    "\n",
    "    X.append(i)\n",
    "\n",
    "train_l=0\n",
    "train_r=0\n",
    "av=0\n",
    "#Training 7-fold\n",
    "for q in range(1,8):\n",
    "    spam = defaultdict()\n",
    "    ham = defaultdict()\n",
    "    spamt = defaultdict()\n",
    "    hamt = defaultdict()\n",
    "    vocab = defaultdict()\n",
    "    train_r = train_l + 1000 // 7\n",
    "    append_list_x = X[0:train_l]\n",
    "    append_list_y = Y[0:train_l]\n",
    "    trainX = X[train_r:] + append_list_x\n",
    "    trainY = Y[train_r:] + append_list_y\n",
    "    testX = X[train_l:train_r]\n",
    "    testY = Y[train_l:train_r]\n",
    "    cs, ch = 0, 0\n",
    "#Calculating frequenices\n",
    "    for (i, j) in zip(trainX, trainY):\n",
    "        l = re.split(' ', i)\n",
    "        if (j == '0'):\n",
    "            cs += 1\n",
    "            for k in l:\n",
    "                if (k not in stopwords) and len(k) > 2:\n",
    "                    if k not in spam:\n",
    "                        spam[k] = 1\n",
    "                    else:\n",
    "                        spam[k] += 1\n",
    "\n",
    "                    if k not in vocab:\n",
    "                        vocab[k] = 1\n",
    "                    else:\n",
    "                        vocab[k] += 1\n",
    "        else:\n",
    "            ch += 1\n",
    "            for k in l:\n",
    "\n",
    "                if (k not in stopwords) and len(k) > 2:\n",
    "                    if k not in ham:\n",
    "                        ham[k] = 1\n",
    "                    else:\n",
    "                        ham[k] += 1\n",
    "\n",
    "                    if k not in vocab:\n",
    "                        vocab[k] = 1\n",
    "                    else:\n",
    "                        vocab[k] += 1\n",
    "\n",
    "    # probspam and probham are probabilities of message being spam or not\n",
    "    probspam = cs / (cs + ch)\n",
    "    probham = 1 - probspam\n",
    "\n",
    "\n",
    "    # Calculating prior probabilities\n",
    "\n",
    "    for k, v in spam.items():\n",
    "        spam[k] = spam[k] / cs\n",
    "\n",
    "    for k, v in ham.items():\n",
    "        ham[k] = ham[k] / ch\n",
    "\n",
    "    Y_pred = []\n",
    "    for i in testX:\n",
    "        l = re.split(' ', i)\n",
    "        pspam = probspam\n",
    "        pham = probham\n",
    "        for j in l:\n",
    "            if j in spam:\n",
    "                pspam = pspam * spam[j]\n",
    "            else:\n",
    "                pspam = pspam * (1 / (cs+len(vocab)))\n",
    "            if j in ham:\n",
    "                pham = pham * ham[j]\n",
    "            else:\n",
    "                pham = pham * (1 / (ch+len(vocab)))\n",
    "\n",
    "        if pspam > pham:\n",
    "            Y_pred.append('0')\n",
    "        else:\n",
    "            Y_pred.append('1')\n",
    "\n",
    "    sum = 0\n",
    "    for x, y in zip(testY, Y_pred):\n",
    "        if x == y:\n",
    "            sum += 1\n",
    "    #printing accuracy\n",
    "    print(\"FOLD\",q,\"accuracy\",sum / np.shape(testY)[0] * 100)\n",
    "    av+=(sum / np.shape(testY)[0]) * 100\n",
    "    train_l = train_l + 1 + 1000 // 7\n",
    "print(\"The average accuracy is \",av/7)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bef29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
